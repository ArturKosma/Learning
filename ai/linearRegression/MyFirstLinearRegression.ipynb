{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# This program is a simple implementation of a linear regression (y = wx + b), where we use \n# mean squared error algorithm to calculate the cost of the function\n# by taking its partial derivatives for the gradient descent to figure out w(Weight) and b(Bias).\n\n# Import numpy (commonly used library for Machine Learning math).\nimport numpy\n\ndef train_linear_regression(InDataSet_Inputs, InDataSet_Outputs, InStep, InIterations):\n    # Input:\n    # InDataSet_Input [numpy.array] - Training inputs.\n    # InDataSet_Outputs [numpy.array] - Training outputs.\n    # InStep [scalar] - Alpha step in linear regression.\n    # InIterations [scalar] - Gradient descent iterations.\n    # Output:\n    # OutWeight [scalar] - Output weight which can be used in the linear regression prediction.\n    # OutBias [scalar] - Output bias which can be used in the linear regression prediction.\n    \n    # Define number of records for training.\n    m = InDataSet_Inputs.shape[0]\n    \n    # Initialize return values weight and bias.\n    OutWeight = 0\n    OutBias = 0\n    \n    # Training loop.\n    for i in range(InIterations):\n        \n        # Find partial derivatives over dw and db used in gradient descent.\n        sum_dw = 0;\n        sum_db = 0;\n        \n        for n in range(m - 1):\n            sum_dw += ((OutWeight * InDataSet_Inputs[n]) + OutBias - InDataSet_Outputs[n]) * InDataSet_Inputs[n]\n            sum_db += ((OutWeight * InDataSet_Inputs[n]) + OutBias - InDataSet_Outputs[n])\n            \n        dw = sum_dw / m\n        db = sum_db / m\n        \n        # Find weight and bias via gradient descent.\n        temp_OutWeight = OutWeight - (InStep * dw)\n        temp_OutBias = OutBias - (InStep * db)\n        \n        OutWeight = temp_OutWeight\n        OutBias = temp_OutBias\n    \n    return OutWeight, OutBias\n\n# Define our training dataset - temperature by ice creams sold.\nicecreams_sold = numpy.array([10.0, 12.0, 14.0, 17.0, 21.0, 23.0, 28.0, 35.0])\ncelsius_temperature = numpy.array([18.0, 22.0, 26.0, 36.0, 40.0, 48.0, 54.0, 71.0])\n\nw, b = train_linear_regression(icecreams_sold, celsius_temperature, 0.005, 10000)\n\nprediction = w * 19 + b\n\nprint(f\"{prediction}\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "text": "37.22210019320406\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}